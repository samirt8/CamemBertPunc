{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from transformers import CamembertTokenizer\n",
    "import torch\n",
    "from torch import nn\n",
    "%matplotlib inline\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "from model import BertPunc\n",
    "from data import load_file, preprocess_data, create_data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('models/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'models/20190418_211742/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = load_file('train_clean_subset.txt')\n",
    "#data_test_asr = load_file('/home/stanfous/datasets/punctuation_model/donald_trump_original_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(path+'hyperparameters.json', 'r') as f:\n",
    "#    hyperparameters = json.load(f)\n",
    "#hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base', do_lower_case=True)\n",
    "\n",
    "punctuation_enc = {\n",
    "    'O': 0,\n",
    "    ',COMMA': 1,\n",
    "    '.PERIOD': 2,\n",
    "    '?QUESTIONMARK': 3,\n",
    "    ':COLON': 4,\n",
    "    '!EXCLAMATIONMARK': 5,\n",
    "    ';SEMICOLON': 6\n",
    "}\n",
    "\n",
    "# punctuation_enc = {\n",
    "#     'O': 0,\n",
    "#     'PERIOD': 1\n",
    "# }\n",
    "\n",
    "#segment_size = hyperparameters['segment_size']\n",
    "segment_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = preprocess_data(data_test, tokenizer, punctuation_enc, segment_size)\n",
    "#X_test_asr, y_test_asr = preprocess_data(data_test_asr, tokenizer, punctuation_enc, segment_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(punctuation_enc)\n",
    "dropout = 0.3\n",
    "bert_punc = nn.DataParallel(BertPunc(segment_size, output_size, dropout).cuda())\n",
    "bert_punc = BertPunc(segment_size, output_size, dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "progress = pd.read_csv('progress.csv', delimiter=';')\n",
    "progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[['training loss', 'loss']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress[['accuracy', 'f1_space', 'f1_comma', 'f1_period', 'f1_question']].plot();\n",
    "# progress[['accuracy', 'f1_O', 'f1_PERIOD']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_punc.load_state_dict(torch.load(path+'model'))\n",
    "bert_punc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "data_loader_test = create_data_loader(X_test, y_test, False, batch_size)\n",
    "#data_loader_test_asr = create_data_loader(X_test_asr, y_test_asr, False, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(data_loader):\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for inputs, labels in tqdm(data_loader, total=len(data_loader)):\n",
    "        with torch.no_grad():\n",
    "            inputs, labels = inputs.cpu(), labels.cpu()\n",
    "            output = bert_punc(inputs)\n",
    "            y_pred += list(output.argmax(dim=1).cpu().data.numpy().flatten())\n",
    "            y_true += list(labels.cpu().data.numpy().flatten())\n",
    "    return y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_test):\n",
    "    precision, recall, f1, _ = metrics.precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=None, labels=[1, 2, 3, 4, 5, 6])\n",
    "    overall = metrics.precision_recall_fscore_support(\n",
    "        y_test, y_pred, average='macro', labels=[1, 2, 3, 4, 5, 6])\n",
    "    result = pd.DataFrame(\n",
    "        np.array([precision, recall, f1]), \n",
    "        columns=list(punctuation_enc.keys())[1:], \n",
    "        index=['Precision', 'Recall', 'F1']\n",
    "    )\n",
    "    result['OVERALL'] = overall[:3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluation(y_pred, y_test):\n",
    "#     precision, recall, f1, _ = metrics.precision_recall_fscore_support(\n",
    "#         y_test, y_pred, average=None, labels=[1])\n",
    "#     overall = metrics.precision_recall_fscore_support(\n",
    "#         y_test, y_pred, average='macro', labels=[1])\n",
    "#     result = pd.DataFrame(\n",
    "#         np.array([precision, recall, f1]), \n",
    "#         columns=list(punctuation_enc.keys())[1:], \n",
    "#         index=['Precision', 'Recall', 'F1']\n",
    "#     )\n",
    "#     result['OVERALL'] = overall[:3]\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test, y_true_test = predictions(data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test = evaluation(y_pred_test, y_true_test)\n",
    "eval_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF\n",
    "ref_test = pd.DataFrame({\n",
    "    'COMMA':    [0.655, 0.471, 0.548],\n",
    "    'PERIOD':   [0.733, 0.725, 0.729],\n",
    "    'QUESTION': [0.707, 0.630, 0.667],\n",
    "    'OVERALL':  [0.700, 0.597, 0.644]\n",
    "}, index=['Precision', 'Recall', 'F1'])\n",
    "ref_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ref_test.columns:\n",
    "    pd.DataFrame({'Reference': ref_test[col], 'BertPunc': eval_test[col]}).plot.bar(\n",
    "        title=col, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_asr, y_true_test_asr = predictions(data_loader_test_asr)\n",
    "eval_test_asr = evaluation(y_pred_test_asr, y_true_test_asr)\n",
    "eval_test_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1517.PDF\n",
    "ref_test_asr = pd.DataFrame({\n",
    "    'COMMA':    [0.596, 0.429, 0.499],\n",
    "    'PERIOD':   [0.707, 0.720, 0.714],\n",
    "    'QUESTION': [0.607, 0.486, 0.540],\n",
    "    'OVERALL':  [0.660, 0.573, 0.614]\n",
    "}, index=['Precision', 'Recall', 'F1'])\n",
    "ref_test_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ref_test_asr.columns:\n",
    "    pd.DataFrame({'Reference': ref_test_asr[col], 'BertPunc': eval_test_asr[col]}).plot.bar(\n",
    "        title=col, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
